name: Process Awin CSV

on:
  # Manual trigger
  workflow_dispatch:
    inputs:
      batch_size:
        description: 'Batch size for processing'
        required: false
        default: '50'
        type: number
      max_products:
        description: 'Maximum products to process (0 = no limit)'
        required: false
        default: '0'
        type: number
      force_full_run:
        description: 'Force full dataset processing'
        required: false
        default: false
        type: boolean

  # Daily at midnight UTC
  schedule:
    - cron: '0 0 * * *'

jobs:
  process-csv:
    runs-on: ubuntu-latest
    timeout-minutes: 720  # 12 hours max

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Set processing parameters
      id: params
      run: |
        if [ "${{ github.event_name }}" = "schedule" ]; then
          echo "Scheduled run - processing full dataset"
          echo "batch_size=50" >> $GITHUB_OUTPUT
          echo "max_products=0" >> $GITHUB_OUTPUT
        else
          echo "Manual run with custom parameters"
          echo "batch_size=${{ inputs.batch_size }}" >> $GITHUB_OUTPUT
          echo "max_products=${{ inputs.max_products }}" >> $GITHUB_OUTPUT
        fi

    - name: Download CSV data
      run: |
        echo "Checking for CSV files in GitHub releases..."

        # First check if there's a latest release
        RELEASE_DATA=$(curl -s https://api.github.com/repos/${{ github.repository }}/releases/latest)

        # Check if the API returned valid data
        if [ "$(echo "$RELEASE_DATA" | jq -r '.message // empty')" = "Not Found" ]; then
          echo "‚ùå No releases found in repository. Please create a release and upload CSV files."
          exit 1
        fi

        # Extract all CSV URLs matching the pattern
        CSV_URLS=$(echo "$RELEASE_DATA" | jq -r '.assets[]? | select(.name | test("datafeed_2525445.*\\.csv$")) | .browser_download_url' 2>/dev/null || echo "")

        if [ -z "$CSV_URLS" ]; then
          echo "‚ùå No CSV files matching 'datafeed_2525445*.csv' found in the latest release assets."
          echo "üìã Available assets in latest release:"
          echo "$RELEASE_DATA" | jq -r '.assets[]?.name // "No assets found"' 2>/dev/null || echo "  - Unable to list assets"
          echo ""
          echo "üì§ Please upload CSV files with names like:"
          echo "   - datafeed_2525445-1 -by MaxAI.csv"
          echo "   - datafeed_2525445-2 -by MaxAI.csv"
          echo "   - etc."
          exit 1
        fi

        # Download all CSV files
        CSV_FILES=""
        echo "$CSV_URLS" | while read -r CSV_URL; do
          if [ -n "$CSV_URL" ] && [ "$CSV_URL" != "null" ]; then
            FILENAME=$(basename "$CSV_URL")
            echo "üì• Downloading $FILENAME..."
            wget -O "$FILENAME" "$CSV_URL"

            # Verify download
            if [ ! -f "$FILENAME" ]; then
              echo "‚ùå Failed to download $FILENAME"
              exit 1
            fi

            FILE_SIZE=$(stat -f%z "$FILENAME" 2>/dev/null || stat -c%s "$FILENAME" 2>/dev/null || echo "unknown")
            echo "‚úÖ Downloaded $FILENAME (Size: $FILE_SIZE bytes)"

            # Add to list of files to process
            CSV_FILES="$CSV_FILES $FILENAME"
          fi
        done

        # Store the list of CSV files for the next step
        echo "CSV_FILES=$CSV_FILES" >> $GITHUB_ENV
        echo "‚úÖ All CSV files downloaded successfully"

    - name: Create .env file
      run: |
        echo "SUPABASE_URL=${{ secrets.SUPABASE_URL }}" >> .env
        echo "SUPABASE_KEY=${{ secrets.SUPABASE_KEY }}" >> .env

    - name: Run processing script
      env:
        BATCH_SIZE: ${{ steps.params.outputs.batch_size }}
        MAX_PRODUCTS: ${{ steps.params.outputs.max_products }}
        CSV_FILES: ${{ env.CSV_FILES }}
      run: |
        echo "Starting processing with batch_size=${{ steps.params.outputs.batch_size }}, max_products=${{ steps.params.outputs.max_products }}"
        echo "CSV files to process: $CSV_FILES"

        # Process each CSV file
        for csv_file in $CSV_FILES; do
          echo "üîÑ Processing $csv_file..."
          CSV_FILE="$csv_file" python process_awin_csv.py
          echo "‚úÖ Completed processing $csv_file"
        done

        echo "üéâ All CSV files processed successfully"

    - name: Upload logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: processing-logs-${{ github.run_id }}
        path: |
          awin_processor*.log
          *.log
        retention-days: 7

    - name: Notify completion
      if: always()
      run: |
        STATUS="${{ job.status }}"
        RUN_TYPE="${{ github.event_name }}"
        TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M:%S UTC")

        echo "Processing completed at $TIMESTAMP"
        echo "Status: $STATUS"
        echo "Run type: $RUN_TYPE"
        echo "Run ID: ${{ github.run_id }}"

        # You can add webhook notifications here
        # Example for Slack/Discord webhook:
        # curl -X POST -H 'Content-type: application/json' \
        #   --data "{\"text\":\"Awin processing $STATUS at $TIMESTAMP (Run: $RUN_TYPE)\"}" \
        #   ${{ secrets.NOTIFICATION_WEBHOOK_URL }}

        if [ "$STATUS" = "success" ]; then
          echo "‚úÖ Processing completed successfully"
        else
          echo "‚ùå Processing failed - check logs for details"
        fi

